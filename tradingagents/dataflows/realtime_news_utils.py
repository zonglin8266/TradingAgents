#!/usr/bin/env python3
"""
å®æ—¶æ–°é—»æ•°æ®è·å–å·¥å…·
è§£å†³æ–°é—»æ»åæ€§é—®é¢˜
"""

import requests
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import os
from dataclasses import dataclass


@dataclass
class NewsItem:
    """æ–°é—»é¡¹ç›®æ•°æ®ç»“æ„"""
    title: str
    content: str
    source: str
    publish_time: datetime
    url: str
    urgency: str  # high, medium, low
    relevance_score: float


class RealtimeNewsAggregator:
    """å®æ—¶æ–°é—»èšåˆå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'TradingAgents-CN/1.0'
        }
        
        # APIå¯†é’¥é…ç½®
        self.finnhub_key = os.getenv('FINNHUB_API_KEY')
        self.alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')
        self.newsapi_key = os.getenv('NEWSAPI_KEY')
        
    def get_realtime_stock_news(self, ticker: str, hours_back: int = 6) -> List[NewsItem]:
        """
        è·å–å®æ—¶è‚¡ç¥¨æ–°é—»
        ä¼˜å…ˆçº§ï¼šä¸“ä¸šAPI > æ–°é—»API > æœç´¢å¼•æ“
        """
        all_news = []
        
        # 1. FinnHubå®æ—¶æ–°é—» (æœ€é«˜ä¼˜å…ˆçº§)
        finnhub_news = self._get_finnhub_realtime_news(ticker, hours_back)
        all_news.extend(finnhub_news)
        
        # 2. Alpha Vantageæ–°é—»
        av_news = self._get_alpha_vantage_news(ticker, hours_back)
        all_news.extend(av_news)
        
        # 3. NewsAPI (å¦‚æœé…ç½®äº†)
        if self.newsapi_key:
            newsapi_news = self._get_newsapi_news(ticker, hours_back)
            all_news.extend(newsapi_news)
        
        # 4. ä¸­æ–‡è´¢ç»æ–°é—»æº
        chinese_news = self._get_chinese_finance_news(ticker, hours_back)
        all_news.extend(chinese_news)
        
        # å»é‡å’Œæ’åº
        unique_news = self._deduplicate_news(all_news)
        return sorted(unique_news, key=lambda x: x.publish_time, reverse=True)
    
    def _get_finnhub_realtime_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–FinnHubå®æ—¶æ–°é—»"""
        if not self.finnhub_key:
            return []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours_back)
            
            # FinnHub APIè°ƒç”¨
            url = "https://finnhub.io/api/v1/company-news"
            params = {
                'symbol': ticker,
                'from': start_time.strftime('%Y-%m-%d'),
                'to': end_time.strftime('%Y-%m-%d'),
                'token': self.finnhub_key
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            news_data = response.json()
            news_items = []
            
            for item in news_data:
                # æ£€æŸ¥æ–°é—»æ—¶æ•ˆæ€§
                publish_time = datetime.fromtimestamp(item.get('datetime', 0))
                if publish_time < start_time:
                    continue
                
                # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                urgency = self._assess_news_urgency(item.get('headline', ''), item.get('summary', ''))
                
                news_items.append(NewsItem(
                    title=item.get('headline', ''),
                    content=item.get('summary', ''),
                    source=item.get('source', 'FinnHub'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('headline', ''), ticker)
                ))
            
            return news_items
            
        except Exception as e:
            print(f"FinnHubæ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _get_alpha_vantage_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–Alpha Vantageæ–°é—»"""
        if not self.alpha_vantage_key:
            return []
        
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': ticker,
                'apikey': self.alpha_vantage_key,
                'limit': 50
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            news_items = []
            
            if 'feed' in data:
                for item in data['feed']:
                    # è§£ææ—¶é—´
                    time_str = item.get('time_published', '')
                    try:
                        publish_time = datetime.strptime(time_str, '%Y%m%dT%H%M%S')
                    except:
                        continue
                    
                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < datetime.now() - timedelta(hours=hours_back):
                        continue
                    
                    urgency = self._assess_news_urgency(item.get('title', ''), item.get('summary', ''))
                    
                    news_items.append(NewsItem(
                        title=item.get('title', ''),
                        content=item.get('summary', ''),
                        source=item.get('source', 'Alpha Vantage'),
                        publish_time=publish_time,
                        url=item.get('url', ''),
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                    ))
            
            return news_items
            
        except Exception as e:
            print(f"Alpha Vantageæ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _get_newsapi_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–NewsAPIæ–°é—»"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            company_names = {
                'AAPL': 'Apple',
                'TSLA': 'Tesla', 
                'NVDA': 'NVIDIA',
                'MSFT': 'Microsoft',
                'GOOGL': 'Google'
            }
            
            query = f"{ticker} OR {company_names.get(ticker, ticker)}"
            
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': query,
                'language': 'en',
                'sortBy': 'publishedAt',
                'from': (datetime.now() - timedelta(hours=hours_back)).isoformat(),
                'apiKey': self.newsapi_key
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            news_items = []
            
            for item in data.get('articles', []):
                # è§£ææ—¶é—´
                time_str = item.get('publishedAt', '')
                try:
                    publish_time = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                except:
                    continue
                
                urgency = self._assess_news_urgency(item.get('title', ''), item.get('description', ''))
                
                news_items.append(NewsItem(
                    title=item.get('title', ''),
                    content=item.get('description', ''),
                    source=item.get('source', {}).get('name', 'NewsAPI'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                ))
            
            return news_items
            
        except Exception as e:
            print(f"NewsAPIæ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _get_chinese_finance_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–ä¸­æ–‡è´¢ç»æ–°é—»"""
        # è¿™é‡Œå¯ä»¥é›†æˆä¸­æ–‡è´¢ç»æ–°é—»API
        # ä¾‹å¦‚ï¼šè´¢è”ç¤¾ã€æ–°æµªè´¢ç»ã€ä¸œæ–¹è´¢å¯Œç­‰
        
        try:
            # ç¤ºä¾‹ï¼šé›†æˆè´¢è”ç¤¾API (éœ€è¦ç”³è¯·)
            # æˆ–è€…ä½¿ç”¨RSSæº
            news_items = []
            
            # è´¢è”ç¤¾RSS (å¦‚æœå¯ç”¨)
            rss_sources = [
                "https://www.cls.cn/api/sw?app=CailianpressWeb&os=web&sv=7.7.5",
                # å¯ä»¥æ·»åŠ æ›´å¤šRSSæº
            ]
            
            for rss_url in rss_sources:
                try:
                    items = self._parse_rss_feed(rss_url, ticker, hours_back)
                    news_items.extend(items)
                except:
                    continue
            
            return news_items
            
        except Exception as e:
            print(f"ä¸­æ–‡è´¢ç»æ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _parse_rss_feed(self, rss_url: str, ticker: str, hours_back: int) -> List[NewsItem]:
        """è§£æRSSæº"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦ä½¿ç”¨feedparseråº“
        return []
    
    def _assess_news_urgency(self, title: str, content: str) -> str:
        """è¯„ä¼°æ–°é—»ç´§æ€¥ç¨‹åº¦"""
        text = (title + ' ' + content).lower()
        
        # é«˜ç´§æ€¥åº¦å…³é”®è¯
        high_urgency_keywords = [
            'breaking', 'urgent', 'alert', 'emergency', 'halt', 'suspend',
            'çªå‘', 'ç´§æ€¥', 'æš‚åœ', 'åœç‰Œ', 'é‡å¤§'
        ]
        
        # ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯
        medium_urgency_keywords = [
            'earnings', 'report', 'announce', 'launch', 'merger', 'acquisition',
            'è´¢æŠ¥', 'å‘å¸ƒ', 'å®£å¸ƒ', 'å¹¶è´­', 'æ”¶è´­'
        ]
        
        if any(keyword in text for keyword in high_urgency_keywords):
            return 'high'
        elif any(keyword in text for keyword in medium_urgency_keywords):
            return 'medium'
        else:
            return 'low'
    
    def _calculate_relevance(self, title: str, ticker: str) -> float:
        """è®¡ç®—æ–°é—»ç›¸å…³æ€§åˆ†æ•°"""
        text = title.lower()
        ticker_lower = ticker.lower()
        
        # åŸºç¡€ç›¸å…³æ€§
        if ticker_lower in text:
            return 1.0
        
        # å…¬å¸åç§°åŒ¹é…
        company_names = {
            'aapl': ['apple', 'iphone', 'ipad', 'mac'],
            'tsla': ['tesla', 'elon musk', 'electric vehicle'],
            'nvda': ['nvidia', 'gpu', 'ai chip'],
            'msft': ['microsoft', 'windows', 'azure'],
            'googl': ['google', 'alphabet', 'search']
        }
        
        if ticker_lower in company_names:
            for name in company_names[ticker_lower]:
                if name in text:
                    return 0.8
        
        return 0.3  # é»˜è®¤ç›¸å…³æ€§
    
    def _deduplicate_news(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """å»é‡æ–°é—»"""
        seen_titles = set()
        unique_news = []
        
        for item in news_items:
            # ç®€å•çš„æ ‡é¢˜å»é‡
            title_key = item.title.lower().strip()
            if title_key not in seen_titles and len(title_key) > 10:
                seen_titles.add(title_key)
                unique_news.append(item)
        
        return unique_news
    
    def format_news_report(self, news_items: List[NewsItem], ticker: str) -> str:
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š"""
        if not news_items:
            return f"æœªè·å–åˆ°{ticker}çš„å®æ—¶æ–°é—»æ•°æ®ã€‚"
        
        # æŒ‰ç´§æ€¥ç¨‹åº¦åˆ†ç»„
        high_urgency = [n for n in news_items if n.urgency == 'high']
        medium_urgency = [n for n in news_items if n.urgency == 'medium']
        low_urgency = [n for n in news_items if n.urgency == 'low']
        
        report = f"# {ticker} å®æ—¶æ–°é—»åˆ†ææŠ¥å‘Š\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ–°é—»æ€»æ•°: {len(news_items)}æ¡\n\n"
        
        if high_urgency:
            report += "## ğŸš¨ ç´§æ€¥æ–°é—»\n\n"
            for news in high_urgency[:3]:  # æœ€å¤šæ˜¾ç¤º3æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"
        
        if medium_urgency:
            report += "## ğŸ“¢ é‡è¦æ–°é—»\n\n"
            for news in medium_urgency[:5]:  # æœ€å¤šæ˜¾ç¤º5æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"
        
        # æ·»åŠ æ—¶æ•ˆæ€§è¯´æ˜
        latest_news = max(news_items, key=lambda x: x.publish_time)
        time_diff = datetime.now() - latest_news.publish_time
        
        report += f"\n## â° æ•°æ®æ—¶æ•ˆæ€§\n"
        report += f"æœ€æ–°æ–°é—»å‘å¸ƒäº: {time_diff.total_seconds() / 60:.0f}åˆ†é’Ÿå‰\n"
        
        if time_diff.total_seconds() < 1800:  # 30åˆ†é’Ÿå†…
            report += "ğŸŸ¢ æ•°æ®æ—¶æ•ˆæ€§: ä¼˜ç§€ (30åˆ†é’Ÿå†…)\n"
        elif time_diff.total_seconds() < 3600:  # 1å°æ—¶å†…
            report += "ğŸŸ¡ æ•°æ®æ—¶æ•ˆæ€§: è‰¯å¥½ (1å°æ—¶å†…)\n"
        else:
            report += "ğŸ”´ æ•°æ®æ—¶æ•ˆæ€§: ä¸€èˆ¬ (è¶…è¿‡1å°æ—¶)\n"
        
        return report


def get_realtime_stock_news(ticker: str, curr_date: str, hours_back: int = 6) -> str:
    """
    è·å–å®æ—¶è‚¡ç¥¨æ–°é—»çš„ä¸»è¦æ¥å£å‡½æ•°
    """
    aggregator = RealtimeNewsAggregator()
    
    try:
        # è·å–å®æ—¶æ–°é—»
        news_items = aggregator.get_realtime_stock_news(ticker, hours_back)
        
        # æ ¼å¼åŒ–æŠ¥å‘Š
        report = aggregator.format_news_report(news_items, ticker)
        
        return report
        
    except Exception as e:
        return f"""
å®æ—¶æ–°é—»è·å–å¤±è´¥ - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âŒ é”™è¯¯ä¿¡æ¯: {str(e)}

ğŸ’¡ å¤‡ç”¨å»ºè®®:
1. æ£€æŸ¥APIå¯†é’¥é…ç½® (FINNHUB_API_KEY, NEWSAPI_KEY)
2. ä½¿ç”¨åŸºç¡€æ–°é—»åˆ†æä½œä¸ºå¤‡é€‰
3. å…³æ³¨å®˜æ–¹è´¢ç»åª’ä½“çš„æœ€æ–°æŠ¥é“
4. è€ƒè™‘ä½¿ç”¨ä¸“ä¸šé‡‘èç»ˆç«¯è·å–å®æ—¶æ–°é—»

æ³¨: å®æ—¶æ–°é—»è·å–ä¾èµ–å¤–éƒ¨APIæœåŠ¡çš„å¯ç”¨æ€§ã€‚
"""
